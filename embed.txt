# tree structure of directory `embed`
ðŸ“ cli/embed/
â”œâ”€â”€ ðŸ“„ __init__.py
â”œâ”€â”€ ðŸ“„ course.py
â”œâ”€â”€ ðŸ“„ discourse.py
â””â”€â”€ ðŸ“„ utils.py

# File Contents

# cli/embed/__init__.py
import rich_click as click

from cli.embed import course
from cli.embed import discourse


@click.group(name="embed", help="Embedding cli")
def cli() -> None:
    pass


cli.add_command(course.embed, name="course")
cli.add_command(discourse.embed, name="discourse")

if __name__ == "__main__":
    cli()

# cli/embed/course.py
# cli/embed/course.py
import asyncio
import re
from pathlib import Path
from typing import List

import rich_click as click
from markdown_it import MarkdownIt
from pydantic import BaseModel
from pydantic import Field
from rich.console import Console
from rich.progress import BarColumn
from rich.progress import Progress
from rich.progress import SpinnerColumn
from rich.progress import TaskProgressColumn
from rich.progress import TextColumn
from rich.progress import TimeElapsedColumn
from rich.progress import MofNCompleteColumn

from cli.embed.utils import EmbeddingChunk
from cli.embed.utils import OpenAIEmbedder
from cli.embed.utils import Settings
from cli.embed.utils import URLAwareTextSplitter
from cli.embed.utils import save_embeddings
from cli.utils import error_exit

console = Console()


class MarkdownSection(BaseModel):
    content: str
    headers: List[str] = Field(default_factory=list)
    level: int = 0
    file_path: str


def parse_markdown(content: str, file_path: str) -> List[MarkdownSection]:
    """Parse markdown into hierarchical sections"""
    md = MarkdownIt("commonmark", {"breaks": True, "html": True})
    tokens = md.parse(content)
    lines = content.split("\n")

    sections = []
    headers = [""] * 6  # h1-h6 tracking
    current_content = []
    line_idx = 0

    for token in tokens:
        if token.type == "heading_open":
            # Save previous section
            if current_content:
                active_headers = [h for h in headers if h]
                sections.append(
                    MarkdownSection(
                        content="\n".join(current_content).strip(),
                        headers=active_headers.copy(),
                        level=len(active_headers),
                        file_path=file_path,
                    )
                )
                current_content = []

            # Update header hierarchy
            level = int(token.tag[1]) - 1
            next_idx = tokens.index(token) + 1
            header_text = (
                tokens[next_idx].content
                if next_idx < len(tokens) and tokens[next_idx].type == "inline"
                else ""
            )

            headers[level:] = [header_text] + [""] * (5 - level)

        # Collect content lines
        if token.map and line_idx < token.map[1]:
            current_content.extend(lines[line_idx : token.map[1]])
            line_idx = token.map[1]

    # Add remaining content
    current_content.extend(lines[line_idx:])
    if current_content:
        active_headers = [h for h in headers if h]
        sections.append(
            MarkdownSection(
                content="\n".join(current_content).strip(),
                headers=active_headers.copy(),
                level=len(active_headers),
                file_path=file_path,
            )
        )

    return [s for s in sections if s.content.strip()]


async def process_file(
    file_path: Path,
    embedder: OpenAIEmbedder,
    splitter: URLAwareTextSplitter,
    progress: Progress = None,
) -> List[EmbeddingChunk]:
    """Process single markdown file"""
    try:
        content = file_path.read_text(encoding="utf-8")
        sections = parse_markdown(content, str(file_path))
    except Exception as e:
        console.print(f"[red]Error reading {file_path}: {e}[/red]")
        return []

    if not sections:
        return []

    chunk_task = progress.add_task(
        f"\t\t[yellow]Processing chunks of {file_path.stem}...",
        total=sum(len(sections)),
    )
    chunks = []
    console.print(f"[cyan]ðŸ“„ {file_path.name} ({len(sections)} sections)[/cyan]")

    for i, section in enumerate(sections):
        if not section.content.strip():
            if progress and chunk_task:
                progress.update(chunk_task, advance=1)
            continue

        topic_path = " > ".join(section.headers) if section.headers else "Root"
        full_text = f"""Topic: {topic_path}
{section.content}"""

        text_chunks = splitter.split_text(full_text)
        console.print(f"  ðŸ“ Section {i + 1} ({topic_path}): {len(text_chunks)} chunks")

        for _, chunk_text in enumerate(text_chunks):
            embedding = await embedder.embed_text(chunk_text)
            if embedding:
                chunks.append(
                    EmbeddingChunk(
                        text=chunk_text,
                        metadata={"topic_path": topic_path},
                        embedding=embedding,
                    )
                )

        if progress and chunk_task:
            progress.update(chunk_task, advance=1)

    return chunks


@click.group(name="course", help="Embed course markdown data")
def cli() -> None:
    pass


@cli.command()
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(exists=True, path_type=Path),
    default="data/course",
    help="Directory with markdown files",
)
@click.option(
    "--output-file",
    "-o",
    type=click.Path(path_type=Path),
    default="embeddings/course.npz",
    help="Output embeddings file",
)
@click.option("--chunk-size", default=1500, help="Max chunk size")
@click.option("--chunk-overlap", default=200, help="Chunk overlap")
def embed(
    input_dir: Path, output_file: Path, chunk_size: int, chunk_overlap: int
) -> None:
    """Embed course markdown files using Gemini"""

    try:
        settings = Settings()
    except Exception as e:
        error_exit(f"Config error: {e}")

    output_file.parent.mkdir(parents=True, exist_ok=True)

    async def run():
        console.print("[bold blue]ðŸ” Analyzing input directory...[/bold blue]")

        md_files = list(input_dir.glob("**/*.md"))
        if not md_files:
            error_exit(f"No markdown files in {input_dir}")

        # Calculate total size
        total_size = sum(f.stat().st_size for f in md_files) / (1024 * 1024)

        # Show summary
        console.print("\n[bold yellow]ðŸ“‹ Embedding Summary[/bold yellow]")
        console.print(f"ðŸ“ Input directory: {input_dir}")
        console.print(f"ðŸ“„ Files to process: {len(md_files)}")
        console.print(f"ðŸ“ Total size: {total_size:.2f} MB")
        console.print(f"âš™ï¸  Chunk size: {chunk_size} chars")
        console.print(f"ðŸ”— Chunk overlap: {chunk_overlap} chars")
        console.print(f"ðŸ’¾ Output: {output_file}")

        if not click.confirm("\nðŸš€ Continue with embedding?", default=True):
            console.print("[yellow]âŒ Cancelled by user[/yellow]")
            return

        console.print("\n[bold blue]ðŸš€ Starting embedding...[/bold blue]")

        embedder = OpenAIEmbedder(settings.api_key)
        splitter = URLAwareTextSplitter(chunk_size, chunk_overlap)

        all_chunks = []
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            MofNCompleteColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
            expand=True,
        ) as progress:
            # Calculate total chunks estimate (rough)

            file_task = progress.add_task(
                "[cyan]Processing files...", total=len(md_files)
            )

            for file_path in md_files:
                progress.update(
                    file_task, description=f"[cyan]Processing {file_path.name}..."
                )

                chunks = await process_file(file_path, embedder, splitter, progress)
                all_chunks.extend(chunks)
                progress.update(file_task, advance=1)

        successful = len([c for c in all_chunks if c.embedding])
        console.print(f"[green]âœ… Embedded {successful} chunks[/green]")

        save_embeddings(all_chunks, output_file)
        size_mb = output_file.stat().st_size / (1024 * 1024)
        console.print(f"[blue]ðŸ’¾ Saved: {output_file} ({size_mb:.2f} MB)[/blue]")

    asyncio.run(run())


if __name__ == "__main__":
    cli()

# cli/embed/discourse.py
import asyncio
import json
from pathlib import Path
from typing import Dict
from typing import List

import rich_click as click
from pydantic import BaseModel
from pydantic import Field
from pydantic import ValidationError
from rich.console import Console
from rich.progress import BarColumn
from rich.progress import Progress
from rich.progress import SpinnerColumn
from rich.progress import TaskProgressColumn
from rich.progress import TextColumn
from rich.progress import TimeElapsedColumn
from rich.progress import MofNCompleteColumn

from cli.embed.utils import EmbeddingChunk
from cli.embed.utils import OpenAIEmbedder
from cli.embed.utils import Settings
from cli.embed.utils import URLAwareTextSplitter
from cli.embed.utils import save_embeddings
from cli.utils import error_exit
from cli.utils import print_error

console = Console()


class PostData(BaseModel):
    username: str
    created_at: str
    content: str
    url: str
    images: List[Dict[str, str]] = Field(default_factory=list)


class TopicData(BaseModel):
    topic_id: int
    topic_title: str
    topic_url: str
    posts: List[PostData]


async def process_json_file(
    file_path: Path,
    embedder: OpenAIEmbedder,
    splitter: URLAwareTextSplitter,
    progress: Progress,
) -> List[EmbeddingChunk]:
    """Process single JSON file"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        topic_data = TopicData(**data)
    except Exception as e:
        print_error(f"Error processing {file_path}: {e}")
        return []

    post_task = progress.add_task(
        f" [yellow]Processing file {file_path.stem}", total=len(topic_data.posts)
    )
    chunks = []
    for post in topic_data.posts:
        if not post.content.strip():
            if progress and post_task:
                progress.update(post_task, advance=1)
            continue
        # Add image context inline
        content = post.content
        for img in post.images:
            if (img_url := img.get("url")) and (context := img.get("context")):
                content = content.replace(img_url, f"{img_url}\n[Image: {context}]")

        text_chunks = splitter.split_text(content)
        
        for chunk_text in text_chunks:
            embedding = await embedder.embed_text(chunk_text)

            if embedding:
                chunks.append(
                    EmbeddingChunk(
                        text=chunk_text,
                        metadata={
                            "topic_title": topic_data.topic_title,
                            "post_url": post.url,
                            "content": post.content,
                        },
                        embedding=embedding,
                    )
                )
        if progress and post_task:
            progress.update(post_task, advance=1)
    return chunks


@click.group(name="discourse", help="Embed Discourse data")
def cli() -> None:
    pass


@cli.command()
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(exists=True, path_type=Path),
    default="data/discourse",
    help="Directory with JSON files",
)
@click.option(
    "--output-file",
    "-o",
    type=click.Path(path_type=Path),
    default="embeddings/discourse.npz",
    help="Output embeddings file",
)
@click.option("--chunk-size", default=1500, help="Max chunk size")
@click.option("--chunk-overlap", default=200, help="Chunk overlap")
def embed(
    input_dir: Path, output_file: Path, chunk_size: int, chunk_overlap: int
) -> None:
    """Embed Discourse JSON files using Gemini"""

    try:
        settings = Settings()
    except ValidationError as e:
        error_exit(f"Config error: {e}")

    output_file.parent.mkdir(parents=True, exist_ok=True)

    async def run():
        console.print("[bold blue]ðŸ” Analyzing input directory...[/bold blue]")

        json_files = list(input_dir.glob("*.json"))
        if not json_files:
            error_exit(f"No JSON files in {input_dir}")

        # Calculate stats
        total_size = sum(f.stat().st_size for f in json_files) / (1024 * 1024)
        total_posts = 0

        for file_path in json_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    total_posts += len(data.get("posts", []))
            except Exception:
                continue

        # Show summary
        console.print("\n[bold yellow]ðŸ“‹ Embedding Summary[/bold yellow]")
        console.print(f"ðŸ“ Input directory: {input_dir}")
        console.print(f"ðŸ“„ JSON files: {len(json_files)}")
        console.print(f"ðŸ’¬ Total posts: {total_posts}")
        console.print(f"ðŸ“ Total size: {total_size:.2f} MB")
        console.print(f"âš™ï¸  Chunk size: {chunk_size} chars")
        console.print(f"ðŸ”— Chunk overlap: {chunk_overlap} chars")
        console.print(f"ðŸ’¾ Output: {output_file}")

        if not click.confirm("\nðŸš€ Continue with embedding?", default=True):
            console.print("[yellow]âŒ Cancelled by user[/yellow]")
            return

        console.print("\n[bold blue]ðŸš€ Starting embedding...[/bold blue]")

        embedder = OpenAIEmbedder(settings.api_key)
        splitter = URLAwareTextSplitter(chunk_size, chunk_overlap)

        all_chunks = []
        with Progress(
            SpinnerColumn(),
            TextColumn(
                "[progress.description]{task.description}"
            ),
            BarColumn(bar_width=60),
            MofNCompleteColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
            expand=True,
        ) as progress:
            task = progress.add_task("[cyan]Processing...", total=len(json_files))

            for file_path in json_files:
                chunks = await process_json_file(
                    file_path, embedder, splitter, progress
                )
                all_chunks.extend(chunks)
                progress.update(task, advance=1)

        successful = len([c for c in all_chunks if c.embedding])
        console.print(f"[green]âœ… Embedded {successful} chunks[/green]")

        save_embeddings(all_chunks, output_file)
        size_mb = output_file.stat().st_size / (1024 * 1024)
        console.print(f"[blue]ðŸ’¾ Saved: {output_file} ({size_mb:.2f} MB)[/blue]")

    asyncio.run(run())


if __name__ == "__main__":
    cli()

# cli/embed/utils.py
import asyncio
import re
import time
from pathlib import Path
from typing import Any
from typing import Optional

import numpy as np
from openai import AsyncOpenAI
from pydantic import BaseModel
from pydantic_settings import BaseSettings
from pydantic_settings import SettingsConfigDict

from cli.utils import error_exit
from cli.utils import print_error
from cli.utils import print_warning


class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")
    api_key: str


class EmbeddingChunk(BaseModel):
    text: str
    metadata: dict[str, Any]
    embedding: Optional[list[float]] = None


class OpenAIEmbedder:
    def __init__(self, api_key: str, model: str = "text-embedding-3-small"):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        self.request_times: list[float] = []

    async def _rate_limit(self) -> None:
        """Rate limiting for OpenAI paid tier (3000 RPM = 50 RPS)"""
        now = time.time()
        self.request_times = [t for t in self.request_times if now - t < 60]

        if len(self.request_times) >= 49:  # Stay under 50 RPS
            wait_time = 60 - (now - self.request_times[0]) + 1
            if wait_time > 0:
                print_warning("\t\t\t\tWaiting for rate limit reset...")
                await asyncio.sleep(wait_time)

        self.request_times.append(now)

    async def embed_text(self, text: str) -> list[float]:
        """Embed text using OpenAI with rate limiting and retries"""
        for attempt in range(3):
            try:
                await self._rate_limit()
                response = await self.client.embeddings.create(
                    model=self.model, input=text
                )
                return response.data[0].embedding

            except Exception as e:
                error_msg = str(e).lower()
                if "429" in error_msg or "rate_limit" in error_msg:
                    wait_time = 15 + (attempt * 10)  # Shorter waits for paid tier
                    print_warning(
                        f"Rate limited (attempt {attempt + 1}/3), waiting {wait_time}s"
                    )
                    await asyncio.sleep(wait_time)
                elif attempt < 2:
                    await asyncio.sleep(2 + (attempt * 2))  # Faster retries
                else:
                    print_error(f"Embedding failed: {str(e)[:80]}")
                    return []
        return []


class URLAwareTextSplitter:
    """Text splitter preserving URLs as atomic units"""

    def __init__(self, chunk_size: int = 3000, chunk_overlap: int = 600):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.url_pattern = re.compile(
            r"(?:https?://[^\s\[\]()]+|ftp://[^\s\[\]()]+|file://[^\s\[\]()]+|"
            r"\[[^\]]*\]\([^)]+\)|<[^>]+>)",
            re.IGNORECASE,
        )

    def split_text(self, text: str) -> list[str]:
        """Split text while preserving URLs"""
        urls = list(self.url_pattern.finditer(text))
        if not urls:
            return self._basic_split(text)

        # Replace URLs with placeholders
        placeholders = {}
        modified_text = text
        offset = 0

        for i, match in enumerate(urls):
            placeholder = f"__URL_{i}__"
            placeholders[placeholder] = match.group()
            start, end = match.start() + offset, match.end() + offset
            modified_text = modified_text[:start] + placeholder + modified_text[end:]
            offset += len(placeholder) - len(match.group())

        # Split and restore URLs
        chunks = self._basic_split(modified_text)
        return [self._restore_urls(chunk, placeholders) for chunk in chunks]

    def _restore_urls(self, chunk: str, placeholders: dict) -> str:
        """Restore URLs in chunk"""
        for placeholder, url in placeholders.items():
            chunk = chunk.replace(placeholder, url)
        return chunk

    def _basic_split(self, text: str) -> list[str]:
        """Basic text splitting with overlap"""
        if len(text) <= self.chunk_size:
            return [text]

        chunks = []
        separators = ["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " "]

        start = 0
        while start < len(text):
            end = min(start + self.chunk_size, len(text))

            # Find best split point
            if end < len(text):
                for sep in separators:
                    split_pos = text.rfind(sep, start, end)
                    if split_pos > start:
                        end = split_pos + len(sep)
                        break

            chunks.append(text[start:end].strip())
            start = max(start + 1, end - self.chunk_overlap)

        return [chunk for chunk in chunks if chunk]


def save_embeddings(chunks: list[EmbeddingChunk], output_path: Path) -> None:
    """Save embeddings to NPZ format"""
    if not chunks:
        error_exit("No chunks to save")

    valid_chunks = [c for c in chunks if c.embedding]
    if not valid_chunks:
        error_exit("No valid embeddings to save")

    # Collect all unique metadata keys
    metadata_keys = set()
    for chunk in valid_chunks:
        metadata_keys.update(chunk.metadata.keys())

    # Build arrays for all metadata fields
    arrays = {
        "embeddings": np.array([c.embedding for c in valid_chunks], dtype=np.float32),
        "texts": np.array([c.text for c in valid_chunks], dtype=object),
    }

    for key in metadata_keys:
        arrays[key] = np.array(
            [c.metadata.get(key, "") for c in valid_chunks], dtype=object
        )

    np.savez_compressed(output_path, **arrays)
