# tree structure of directory `cli`
üìÅ cli
‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îú‚îÄ‚îÄ üìÅ data
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ course.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ discourse.py
‚îú‚îÄ‚îÄ üìÅ embed
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ course.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ discourse copy.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ discourse.py
‚îî‚îÄ‚îÄ üìÅ utils
    ‚îî‚îÄ‚îÄ üìÑ __init__.py

# File Contents

# cli/__init__.py
from pathlib import Path

import rich_click as click
import toml
from rich.console import Console
from rich.prompt import Confirm

from cli import data
from cli import embed
from cli.utils import error_exit
from cli.utils import print_success
from cli.utils import print_table
from cli.utils import print_warning

console = Console()


@click.group(
    name="cli",
    help="TDS Project 1 Data Prepration Tool - command-line interface",
)
def cli() -> None:
    pass


@cli.command("self-update", help="Auto-update [project.scripts] in pyproject.toml")
def self_update() -> None:
    root_dir = Path(__file__).resolve().parent.parent
    cli_dir = root_dir / "cli"
    toml_path = root_dir / "pyproject.toml"

    if not toml_path.exists():
        error_exit("pyproject.toml not found at project root.")

    def discover_scripts() -> dict[str, str]:
        entries = {}
        for path in cli_dir.rglob("*.py"):
            if not path.is_file():
                continue
            source = path.read_text(encoding="utf-8")
            lines = [
                line.strip() for line in source.strip().splitlines() if line.strip()
            ]

            if not (
                len(lines) >= 2
                and lines[-2] == 'if __name__ == "__main__":'
                and "cli()" in lines[-1]
            ):
                continue

            if path.name == "__init__.py":
                rel_path = path.relative_to(cli_dir).parent
                module_parts = rel_path.parts
                if module_parts:
                    script_name = module_parts[-1]
                    fqdn = f"cli.{'.'.join(module_parts)}:cli"
                    entries[script_name] = fqdn
                else:
                    entries["cli"] = "cli:cli"
        return entries

    new_entries = discover_scripts()

    with toml_path.open("r", encoding="utf-8") as f:
        data = toml.load(f)

    project = data.get("project", {})
    existing = project.get("scripts", {})
    added = {k: v for k, v in new_entries.items() if k not in existing}
    removed = {k: v for k, v in existing.items() if k not in new_entries}
    unchanged = {k: v for k, v in existing.items() if k in new_entries}

    # Build rows for table output
    all_rows = [[k, v, "existing"] for k, v in unchanged.items()]

    if added or removed:
        all_rows.append(["", "", ""])  # visual separator

    for k, v in added.items():
        all_rows.append([k, v, "new"])

    for k, v in removed.items():
        all_rows.append([k, v, "removed"])

    column_styles = {"Script": "cyan", "Entry Point": "green", "Status": "dim"}
    print_table(
        title="[project.scripts]",
        columns=["Script", "Entry Point", "Status"],
        rows=all_rows,
        column_styles=column_styles,
        status_column=2,
    )

    if not added and not removed:
        console.print("No changes detected.", style="dim")
        return

    if Confirm.ask("Do you want to apply these changes to pyproject.toml?"):
        updated_scripts = {**unchanged, **added}
        project["scripts"] = updated_scripts
        data["project"] = project

        with toml_path.open("w", encoding="utf-8") as f:
            toml.dump(data, f)

        for k in added:
            print_success(f"Added script: {k}")
        for k in removed:
            print_warning(f"Removed script: {k}")
    else:
        print_warning("Aborted. No changes made.")


cli.add_command(embed.cli)
cli.add_command(data.cli)


if __name__ == "__main__":
    cli()

# cli/data/__init__.py
import rich_click as click

from cli.data import course
from cli.data import discourse


@click.group(name="scrape", help="Scrapping cli")
def cli() -> None:
    pass


cli.add_command(course.cli)
cli.add_command(discourse.cli)

if __name__ == "__main__":
    cli()

# cli/data/course.py
import asyncio
import base64
import re
import time
from pathlib import Path

import rich_click as click
from google import generativeai as genai
from pydantic import ValidationError
from pydantic_settings import BaseSettings
from pydantic_settings import SettingsConfigDict
from rich.console import Console
from rich.progress import BarColumn
from rich.progress import Progress
from rich.progress import SpinnerColumn
from rich.progress import TaskProgressColumn
from rich.progress import TextColumn
from rich.progress import TimeElapsedColumn

from cli.utils import print_error
from cli.utils import print_important
from cli.utils import print_success
from cli.utils import print_warning

console = Console()


class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")
    genai_api_key: str


class AIImageAnalyzer:
    def __init__(self, api_key: str) -> None:
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash-lite")
        self.semaphore = asyncio.Semaphore(2)
        self.request_times: list[float] = []

    async def _rate_limit(self) -> None:
        """Rate limiting for free tier (15 RPM)."""
        now = time.time()
        self.request_times = [t for t in self.request_times if now - t < 60]

        if len(self.request_times) >= 14:
            wait_time = 60 - (now - self.request_times[0]) + 1
            if wait_time > 0:
                print_warning(f"AI rate limit: waiting {wait_time:.1f}s")
                await asyncio.sleep(wait_time)

        await asyncio.sleep(4.5)
        self.request_times.append(now)

    async def analyze_image(self, image_b64: str) -> str | None:
        """Analyze image with AI."""
        async with self.semaphore:
            for attempt in range(3):
                try:
                    await self._rate_limit()

                    image_data = image_b64.split(",")[1] if "," in image_b64 else image_b64
                    image_bytes = base64.b64decode(image_data)

                    prompt = "Describe this course-related image in 2-3 sentences: main subject, educational context, and any important text or technical details."

                    response = await asyncio.to_thread(
                        self.model.generate_content,
                        [prompt, {"mime_type": "image/jpeg", "data": image_bytes}],
                    )

                    return response.text.strip() if response.text else None

                except Exception as e:
                    error_msg = str(e).lower()

                    match error_msg:
                        case msg if "429" in msg or "quota" in msg:
                            wait_time = 15 + (attempt * 10)
                            print_warning(f"Rate limited (attempt {attempt + 1}/3), waiting {wait_time}s")
                            await asyncio.sleep(wait_time)
                        case msg if "400" in msg:
                            return None
                        case _:
                            if attempt < 2:
                                await asyncio.sleep(10 + (attempt * 5))
                                continue
                            print_error(f"AI analysis failed: {str(e)[:80]}")
                            return None

            return None

    async def process_markdown_file(self, md_file: Path) -> bool:
        """Process single markdown file and insert AI context for local images."""
        try:
            content = md_file.read_text(encoding="utf-8")
        except OSError:
            return False

        # Find all images in markdown
        img_pattern = r'!\[([^\]]*)\]\(([^)\s]+)(?:\s+"[^"]*")?\)'
        images = list(re.finditer(img_pattern, content))
        
        if not images:
            return False

        console.print(f"[bold cyan]üìÑ Processing: {md_file.name} ({len(images)} images)[/bold cyan]")
        modified = False

        # Process images in reverse order to maintain string positions
        for match in reversed(images):
            image_url = match.group(2)
            image_url_short = image_url.split("/")[-1][:40] if "/" in image_url else image_url[:40]
            
            console.print(f"  üñºÔ∏è  Processing: {image_url_short}")
            
            # Check if context already exists
            next_content = content[match.end():]
            if re.match(r'\s*\n\s*\*Image Description:', next_content):
                console.print("  ‚úÖ  Context already exists")
                continue
            
            # Skip non-local images
            if (image_url.startswith(('http://', 'https://', 'data:', 'ftp://')) or
                not any(image_url.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp'])):
                print_warning(f"Skipping non-local image: {image_url_short}")
                continue

            # Load local image file
            console.print("  üì•  Loading local image...")
            try:
                image_path = md_file.parent / image_url
                if not image_path.exists():
                    print_warning(f"Local file not found: {image_url_short}")
                    continue

                image_data = image_path.read_bytes()
                ext = image_path.suffix.lower()
                content_type_map = {
                    '.png': 'image/png', '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg',
                    '.gif': 'image/gif', '.bmp': 'image/bmp', '.svg': 'image/svg+xml', '.webp': 'image/webp'
                }
                content_type = content_type_map.get(ext, 'image/png')
                
                b64_data = base64.b64encode(image_data).decode('utf-8')
                image_b64 = f"data:{content_type};base64,{b64_data}"
                
                console.print("  üìÅ  Source: local file")
                console.print("  ü§ñ  AI analyzing...")
                
                context = await self.analyze_image(image_b64)
                
                if context:
                    context_preview = context[:60] + "..." if len(context) > 60 else context
                    print_success(f"AI Result: {context_preview}")
                    
                    # Insert context after the image
                    context_text = f"\n\n*Image Description: {context}*\n"
                    content = content[:match.end()] + context_text + content[match.end():]
                    modified = True
                else:
                    print_warning(f"AI analysis failed for: {image_url_short}")
                    
            except OSError as e:
                print_warning(f"Cannot read file {image_url_short}: {str(e)[:50]}")

        # Save if modified
        if modified:
            md_file.write_text(content, encoding="utf-8")
            print_success(f"Updated markdown file: {md_file.name}")
            return True
        else:
            console.print("  üìù  No changes needed")
            return False

    async def analyze_course_markdown(self, data_dir: Path, progress: Progress, task_id: int) -> None:
        """Analyze course images in markdown files and insert context."""
        markdown_files = list(data_dir.glob("**/*.md"))
        
        # Single pass: read files and filter those with images
        files_with_images = []
        total_images = 0
        
        for md_file in markdown_files:
            try:
                content = md_file.read_text(encoding="utf-8")
                images = re.findall(r'!\[([^\]]*)\]\(([^)\s]+)(?:\s+"[^"]*")?\)', content)
                
                if images:
                    files_with_images.append(md_file)
                    total_images += len(images)
                    
            except OSError:
                continue

        if not files_with_images:
            print_warning("No markdown files with images found")
            return

        progress.update(task_id, total=total_images)
        print_important(f"Found {total_images} images across {len(files_with_images)} markdown files")
        console.print()

        # Process only files that contain images
        for md_file in files_with_images:
            await self.process_markdown_file(md_file)
            console.print()


@click.group(name="course", help="Process course markdown files")
def cli() -> None:
    pass


@cli.command()
def scrape() -> None:
    """Scrape course content (placeholder - not implemented)."""
    print_warning("Course scraping not implemented")
    print_warning("Course content is expected in .md format in the data/course folder")
    print_warning("This command is reserved for future implementation")


@cli.command()
@click.argument("data_dir", default="data/course", type=click.Path(exists=True, file_okay=False, path_type=Path))
def image_context(data_dir: Path) -> None:
    """Analyze local images in markdown files with AI and add context."""
    try:
        settings = Settings()
    except ValidationError as e:
        raise click.ClickException(f"Configuration error: {e}")

    analyzer = AIImageAnalyzer(settings.genai_api_key)

    async def run_analysis() -> None:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task_id = progress.add_task("[cyan]ü§ñ AI Local Image Analysis...", total=None)
            await analyzer.analyze_course_markdown(data_dir, progress, task_id)
            progress.update(task_id, description="[green]‚úÖ AI analysis complete!")

    asyncio.run(run_analysis())


if __name__ == "__main__":
    cli()

# cli/data/discourse.py
#!/usr/bin/env python3.12
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "httpx>=0.27.0",
#     "pydantic>=2.8.0",
#     "pydantic-settings>=2.4.0",
#     "rich-click>=1.8.0",
#     "rich>=13.0.0",
#     "markdownify>=0.11.6",
#     "google-generativeai>=0.3.0",
# ]
# ///
"""Ultra-efficient Discourse topic scraper with async processing."""

import asyncio
import base64
import json
import re
from datetime import datetime
from datetime import timezone
from pathlib import Path
from typing import Any
from urllib.parse import urljoin

import httpx
import rich_click as click
from google import generativeai as genai
from markdownify import markdownify as md
from pydantic import BaseModel
from pydantic import Field
from pydantic_settings import BaseSettings
from pydantic_settings import SettingsConfigDict
from rich.console import Console
from rich.progress import BarColumn
from rich.progress import Progress
from rich.progress import SpinnerColumn
from rich.progress import TaskProgressColumn
from rich.progress import TextColumn
from rich.progress import TimeElapsedColumn

console = Console()


class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env", env_file_encoding="utf-8"
    )

    genai_api_key: str = Field(alias="GENAI_API_KEY")


class CookieConfig(BaseModel):
    t_token: str = Field(alias="_t")
    forum_session: str = Field(alias="_forum_session")


class ImageData(BaseModel):
    url: str
    base64_data: str | None = None
    context: str | None = None


class Post(BaseModel):
    username: str
    created_at: datetime
    content: str
    url: str
    images: list[ImageData] = Field(default_factory=list)


class Topic(BaseModel):
    id: int
    title: str
    created_at: datetime
    last_posted_at: datetime | None = None
    posts_count: int
    views: int
    category_id: int
    url: str = ""
    posts: list[Post] = Field(default_factory=list)


class ScrapingConfig(BaseModel):
    cookies: CookieConfig
    start_date: str = "2025-01-01"
    end_date: str = "2025-04-15"
    output_dir: str = "discourse_data"
    category_id: int = 34
    genai_api_key: str | None = None


def load_config(
    json_file: str, require_api_key: bool = False
) -> ScrapingConfig:
    """Load and validate configuration."""
    try:
        with Path(json_file).open() as f:
            json_data = json.load(f)

        # Get API key from settings only if required for AI commands
        if require_api_key and (
            "genai_api_key" not in json_data or not json_data["genai_api_key"]
        ):
            settings = Settings()
            json_data["genai_api_key"] = settings.genai_api_key

        config = ScrapingConfig.model_validate(json_data)

        # Validate API key is present for AI commands
        if require_api_key and not config.genai_api_key:
            raise click.ClickException(
                "GENAI_API_KEY required for AI analysis. Set in JSON config or environment."
            )

        return config
    except Exception as e:
        console.print_exception()
        raise click.ClickException(f"Config error: {e}")


class DiscourseClient:
    def __init__(self, base_url: str, cookie_config: CookieConfig):
        self.base_url = base_url.rstrip("/")
        self.session = httpx.AsyncClient(
            cookies={
                "_t": cookie_config.t_token,
                "_forum_session": cookie_config.forum_session,
            },
            timeout=30.0,
            limits=httpx.Limits(max_connections=5, max_keepalive_connections=2),
        )
        self.rate_limit_delay = 0.5

    async def _request_with_retry(self, url: str, **kwargs) -> dict[str, Any]:
        """Request with exponential backoff."""
        for attempt in range(3):
            try:
                await asyncio.sleep(self.rate_limit_delay)
                response = await self.session.get(url, **kwargs)

                if response.status_code == 429:
                    self.rate_limit_delay *= 2
                    wait_time = min(self.rate_limit_delay, 10.0)
                    console.print(
                        f"[yellow]Rate limited, waiting {wait_time:.1f}s[/yellow]"
                    )
                    await asyncio.sleep(wait_time)
                    continue

                if response.status_code == 403:
                    raise click.ClickException(
                        "Authentication failed - check cookies"
                    )

                response.raise_for_status()
                self.rate_limit_delay = max(0.5, self.rate_limit_delay * 0.9)
                return response.json()

            except httpx.HTTPStatusError as e:
                if attempt == 2:
                    raise click.ClickException(
                        f"HTTP {e.response.status_code}: {e.response.text[:200]}"
                    )
                await asyncio.sleep(2**attempt)

        raise click.ClickException("Max retries exceeded")

    async def get_category_topics(
        self, category_id: int, page: int = 0
    ) -> list[Topic]:
        """Get topics from category with modern endpoints."""
        # Try multiple endpoint patterns for compatibility
        endpoints = [
            (
                f"{self.base_url}/c/{category_id}/l/latest.json",
                {"page": page} if page > 0 else {},
            ),
            (
                f"{self.base_url}/latest.json",
                (
                    {"category": category_id, "page": page}
                    if page > 0
                    else {"category": category_id}
                ),
            ),
        ]

        for url, params in endpoints:
            try:
                data = await self._request_with_retry(url, params=params)
                break
            except click.ClickException:
                continue
        else:
            raise click.ClickException("Failed to access category topics")

        topics = []
        for topic_data in data.get("topic_list", {}).get("topics", []):
            topic_id = topic_data["id"]
            topics.append(
                Topic(
                    id=topic_id,
                    title=topic_data["title"],
                    created_at=datetime.fromisoformat(
                        topic_data["created_at"].replace("Z", "+00:00")
                    ),
                    last_posted_at=(
                        datetime.fromisoformat(
                            topic_data["last_posted_at"].replace("Z", "+00:00")
                        )
                        if topic_data.get("last_posted_at")
                        else None
                    ),
                    posts_count=topic_data["posts_count"],
                    views=topic_data["views"],
                    category_id=topic_data["category_id"],
                    url=f"{self.base_url}/t/{topic_id}",
                )
            )
        return topics

    async def get_topic_posts(self, topic_id: int) -> list[Post]:
        """Get all posts for a topic."""
        url = f"{self.base_url}/t/{topic_id}/posts.json"
        data = await self._request_with_retry(url)

        posts = []
        for post_data in data.get("post_stream", {}).get("posts", []):
            html_content = post_data["cooked"]

            # Extract all images with context from HTML content
            images = await self._extract_images_with_context(html_content)

            # Convert HTML to Markdown
            markdown_content = md(html_content, heading_style="ATX").strip()

            posts.append(
                Post(
                    username=post_data["username"],
                    created_at=datetime.fromisoformat(
                        post_data["created_at"].replace("Z", "+00:00")
                    ),
                    content=markdown_content,
                    url=f"{self.base_url}/t/{topic_id}/{post_data['post_number']}",
                    images=images,
                )
            )
        return posts

    async def _extract_images_with_context(self, html: str) -> list[ImageData]:
        """Extract all images from HTML with surrounding context, excluding avatars."""
        # Find all img tags with their surrounding context
        img_pattern = r'<p[^>]*>([^<]*)<img[^>]+src=["\']([^"\']+)["\'][^>]*>([^<]*)</p>|<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
        matches = re.findall(img_pattern, html, re.IGNORECASE | re.DOTALL)

        images = []
        for match in matches:
            # Extract URL and context from regex groups
            if match[1]:  # Image within paragraph
                url = match[1]
                context = (match[0] + match[2]).strip()
            else:  # Standalone image
                url = match[3]
                context = None
            # Add this at class level in DiscourseClient.__init__:
            self.skip_image_pattern = re.compile(
                r"(avatars\.discourse-cdn\.com|emoji\.discourse-cdn\.com|user_avatar|"
                r"gravatar\.com|letter_avatar_proxy|/images/emoji/|\.svg$)",
                re.IGNORECASE,
            )

            # Then in the method:
            if self.skip_image_pattern.search(url):
                continue
            # Get base64 data
            base64_data = await self.get_image_as_base64(url)

            images.append(
                ImageData(
                    url=url,
                    base64_data=base64_data,
                    context=context if context else None,
                )
            )

        return images

    async def get_image_as_base64(self, image_url: str) -> str | None:
        """Download image and return as base64 data URL."""
        try:
            if not image_url.startswith("http"):
                image_url = urljoin(self.base_url, image_url)

            response = await self.session.get(image_url)
            response.raise_for_status()

            # Get content type
            content_type = response.headers.get("content-type", "image/png")

            # Encode to base64
            b64_data = base64.b64encode(response.content).decode("utf-8")
            return f"data:{content_type};base64,{b64_data}"

        except Exception as e:
            console.print(f"[red]Failed to download {image_url}: {e}[/red]")
            return None

    async def close(self):
        await self.session.aclose()


class AIImageAnalyzer:
    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("gemini-2.0-flash-lite")
        self.semaphore = asyncio.Semaphore(2)  # 2 concurrent requests
        self.request_times = []  # Track for rate limiting

    async def _rate_limit(self):
        """Rate limiting for free tier (15 RPM)."""
        now = asyncio.get_event_loop().time()
        self.request_times = [t for t in self.request_times if now - t < 60]

        if len(self.request_times) >= 14:
            wait_time = 60 - (now - self.request_times[0]) + 1
            if wait_time > 0:
                console.print(
                    f"[yellow]‚è≥ AI rate limit: waiting {wait_time:.1f}s[/yellow]"
                )
                await asyncio.sleep(wait_time)

        await asyncio.sleep(4.5)  # Base delay for stability
        self.request_times.append(now)

    async def analyze_image(self, image_b64: str) -> str | None:
        """Analyze image with AI."""
        async with self.semaphore:
            for attempt in range(3):
                try:
                    await self._rate_limit()

                    # Extract base64 data
                    image_data = (
                        image_b64.split(",")[1]
                        if "," in image_b64
                        else image_b64
                    )
                    image_bytes = base64.b64decode(image_data)

                    prompt = "Describe this image in 2-3 sentences: main subject, context/setting, and any important text or technical details."

                    response = await asyncio.to_thread(
                        self.model.generate_content,
                        [
                            prompt,
                            {"mime_type": "image/jpeg", "data": image_bytes},
                        ],
                    )

                    return response.text.strip() if response.text else None

                except Exception as e:
                    error_msg = str(e).lower()

                    if "429" in error_msg or "quota" in error_msg:
                        wait_time = 15 + (attempt * 10)
                        console.print(
                            f"[yellow]‚ö†Ô∏è Rate limited (attempt {attempt + 1}/3), waiting {wait_time}s[/yellow]"
                        )
                        await asyncio.sleep(wait_time)
                        continue
                    elif "400" in error_msg:
                        return None
                    else:
                        if attempt < 2:
                            await asyncio.sleep(10 + (attempt * 5))
                            continue
                        console.print(
                            f"[red]‚ùå AI analysis failed: {str(e)[:80]}[/red]"
                        )
                        return None
            return None

    async def download_and_analyze_images(
        self, output_dir: Path, progress: Progress, task_id: int
    ):
        """Download images and analyze with AI for all topics."""
        topic_files = list(output_dir.glob("topic_*.json"))

        total_images = 0
        processed_images = 0

        # Count total images first
        for topic_file in topic_files:
            with topic_file.open() as f:
                topic_data = json.load(f)

            for post in topic_data["posts"]:
                total_images += len(post.get("images", []))

        progress.update(task_id, total=total_images)
        console.print(
            f"[blue]üîç Found {total_images} images across {len(topic_files)} topics[/blue]\n"
        )

        async with httpx.AsyncClient(timeout=30.0) as session:
            for topic_file in topic_files:
                with topic_file.open() as f:
                    topic_data = json.load(f)

                topic_title = topic_data.get("topic_title", "Unknown Topic")
                topic_id = topic_data.get("topic_id", "Unknown")

                console.print(
                    f"[bold cyan]üìÑ Processing Topic {topic_id}: {topic_title[:80]}[/bold cyan]"
                )

                modified = False

                for post in topic_data["posts"]:
                    for image in post.get("images", []):
                        if image.get("base64_data") and image.get("context"):
                            processed_images += 1
                            progress.update(task_id, advance=1)
                            continue

                        # Log current image being processed
                        image_url_short = image["url"].split("/")[-1][:40]
                        console.print(f"  üñºÔ∏è  Downloading: {image_url_short}")

                        # Download image
                        try:
                            response = await session.get(image["url"])
                            response.raise_for_status()
                            content_type = response.headers.get(
                                "content-type", "image/png"
                            )
                            b64_data = base64.b64encode(
                                response.content
                            ).decode("utf-8")
                            image["base64_data"] = (
                                f"data:{content_type};base64,{b64_data}"
                            )
                            console.print("  ‚úÖ  Downloaded successfully")
                        except Exception as e:
                            console.print(
                                f"  ‚ùå  Download failed: {str(e)[:50]}"
                            )
                            processed_images += 1
                            progress.update(task_id, advance=1)
                            continue

                        # Log AI analysis start
                        console.print("  ü§ñ  AI analyzing...")

                        # Analyze with AI
                        context = await self.analyze_image(image["base64_data"])
                        image["context"] = context
                        modified = True

                        if context:
                            context_preview = (
                                context[:60] + "..."
                                if len(context) > 60
                                else context
                            )
                            console.print(f"  ‚ú®  AI Result: {context_preview}")
                        else:
                            console.print("  ‚ö†Ô∏è   AI analysis failed")

                        processed_images += 1
                        progress.update(task_id, advance=1)

                # Save updated data
                if modified:
                    with topic_file.open("w") as f:
                        json.dump(topic_data, f, indent=2, default=str)
                    console.print(f"  üíæ  Saved updates to {topic_file.name}")

                console.print()  # Add blank line between topics


async def process_topic(
    client: DiscourseClient,
    topic: Topic,
    start_date: datetime,
    end_date: datetime,
    output_dir: Path,
    progress: Progress,
    topic_task: int,
) -> bool:
    """Process single topic: get posts and save as single JSON file."""
    if not (start_date <= topic.created_at <= end_date):
        progress.update(topic_task, advance=1)
        return False

    posts = await client.get_topic_posts(topic.id)

    # Save as single JSON file instead of folder
    topic_data = {
        "topic_id": topic.id,
        "topic_title": topic.title,
        "topic_url": topic.url,
        "posts": [post.model_dump(mode="json") for post in posts],
    }

    (output_dir / f"topic_{topic.id}.json").write_text(
        json.dumps(topic_data, indent=2, default=str)
    )
    progress.update(topic_task, advance=1)
    return True


async def scrape_discourse(config: ScrapingConfig):
    """Main scraping logic."""
    try:
        start_dt = datetime.fromisoformat(config.start_date).replace(
            tzinfo=timezone.utc
        )
        end_dt = datetime.fromisoformat(config.end_date).replace(
            tzinfo=timezone.utc
        )
    except ValueError:
        raise click.ClickException("Invalid date format. Use YYYY-MM-DD")

    base_url = "https://discourse.onlinedegree.iitm.ac.in"
    t_token = config.cookies.t_token
    cookie_masked = (
        f"{t_token[:4]}...{t_token[-4:]}" if len(t_token) > 8 else "****"
    )

    console.print("\n[bold blue]Scraping Plan:[/bold blue]")
    console.print(f"[cyan]URL:[/cyan] {base_url}")
    console.print(f"[cyan]Cookie:[/cyan] {cookie_masked}")
    console.print(f"[cyan]Category:[/cyan] {config.category_id}")
    console.print(
        f"[cyan]Date Range:[/cyan] {config.start_date} to {config.end_date}"
    )
    console.print(f"[cyan]Output:[/cyan] {config.output_dir}\n")

    if not click.confirm("Proceed?"):
        return

    output_path = Path(config.output_dir)
    output_path.mkdir(exist_ok=True)

    client = DiscourseClient(base_url, config.cookies)

    try:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
        ) as progress:

            page_task = progress.add_task(
                "[cyan]Fetching topics...", total=None
            )

            # Fetch all topics in batches
            all_topics = []
            page = 0
            batch_size = 5

            while True:
                page_tasks = [
                    client.get_category_topics(config.category_id, p)
                    for p in range(page, page + batch_size)
                ]
                page_results = await asyncio.gather(
                    *page_tasks, return_exceptions=True
                )

                empty_pages = 0
                for i, result in enumerate(page_results):
                    if isinstance(result, Exception) or not result:
                        empty_pages += 1
                        continue

                    all_topics.extend(result)
                    progress.update(
                        page_task,
                        description=f"[cyan]Fetched {len(all_topics)} topics (page {page + i})",
                    )

                    if result and result[-1].created_at < start_dt:
                        empty_pages = batch_size
                        break

                page += batch_size
                if empty_pages >= batch_size:
                    break

            progress.update(
                page_task,
                completed=True,
                description=f"[green]Found {len(all_topics)} topics",
            )

            # Filter by date
            filtered_topics = [
                t for t in all_topics if start_dt <= t.created_at <= end_dt
            ]
            console.print(
                f"[green]{len(filtered_topics)} topics in date range[/green]"
            )

            if not filtered_topics:
                console.print("[yellow]No topics in date range[/yellow]")
                return

            topic_task = progress.add_task(
                "[blue]Processing topics", total=len(filtered_topics)
            )

            # Process topics concurrently
            semaphore = asyncio.Semaphore(3)

            async def process_with_semaphore(topic):
                async with semaphore:
                    return await process_topic(
                        client,
                        topic,
                        start_dt,
                        end_dt,
                        output_path,
                        progress,
                        topic_task,
                    )

            results = await asyncio.gather(
                *[process_with_semaphore(topic) for topic in filtered_topics]
            )
            console.print(f"[green]Processed {sum(results)} topics[/green]")

    finally:
        await client.close()


@click.group(
    name="discourse", help="Discourse scraper with separated AI analysis"
)
def cli():
    """Discourse scraper with separated AI analysis."""
    click.echo("Welcome to the Discourse Scraper CLI!")


@cli.command()
@click.argument("json_file", type=click.Path(exists=True, readable=True))
def scrape(json_file: str):
    """Scrape discourse topics (without AI analysis)."""
    config = load_config(json_file, require_api_key=False)
    asyncio.run(scrape_discourse(config))


@cli.command()
@click.argument("json_file", type=click.Path(exists=True, readable=True))
def image_context(json_file: str):
    """Analyze images with AI and save context back to JSON files."""
    config = load_config(json_file, require_api_key=True)

    output_path = Path(config.output_dir)
    if not output_path.exists():
        raise click.ClickException(
            f"Output directory {config.output_dir} does not exist. Run 'scrape' first."
        )

    analyzer = AIImageAnalyzer(config.genai_api_key)

    async def run_analysis():
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task_id = progress.add_task(
                "[cyan]ü§ñ AI Image Analysis...", total=None
            )
            await analyzer.download_and_analyze_images(
                output_path, progress, task_id
            )
            progress.update(
                task_id, description="[green]‚úÖ AI analysis complete!"
            )

    asyncio.run(run_analysis())


if __name__ == "__main__":
    cli()

# cli/embed/__init__.py
import rich_click as click

from cli.embed import course
from cli.embed import discourse


@click.group(name="embed", help="Embedding cli")
def cli() -> None:
    pass


cli.add_command(course.embed, name="course")
cli.add_command(discourse.embed, name="discourse")

if __name__ == "__main__":
    cli()

# cli/embed/course.py
# cli/embed/course.py
import asyncio
import re
from pathlib import Path
from typing import List

import numpy as np
import rich_click as click
from markdown_it import MarkdownIt
from pydantic import BaseModel
from pydantic import Field
from pydantic_settings import BaseSettings
from pydantic_settings import SettingsConfigDict
from rich.console import Console
from rich.progress import BarColumn
from rich.progress import Progress
from rich.progress import SpinnerColumn
from rich.progress import TaskProgressColumn
from rich.progress import TextColumn
from rich.progress import TimeElapsedColumn

from cli.embed.discourse import EmbeddingChunk
from cli.embed.discourse import GeminiEmbedder
from cli.utils import error_exit

console = Console()


class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")
    genai_api_key: str


class MarkdownSection(BaseModel):
    content: str
    headers: List[str] = Field(default_factory=list)
    level: int = 0
    file_path: str


class URLAwareTextSplitter:
    """Text splitter preserving URLs as atomic units"""

    def __init__(self, chunk_size: int = 1500, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.url_pattern = re.compile(
            r"(?:https?://[^\s\[\]()]+|ftp://[^\s\[\]()]+|file://[^\s\[\]()]+|"
            r"\[[^\]]*\]\([^)]+\)|<[^>]+>)",
            re.IGNORECASE,
        )

    def split_text(self, text: str) -> List[str]:
        """Split text while preserving URLs"""
        urls = list(self.url_pattern.finditer(text))
        if not urls:
            return self._basic_split(text)

        # Replace URLs with placeholders
        placeholders = {}
        modified_text = text
        offset = 0

        for i, match in enumerate(urls):
            placeholder = f"__URL_{i}__"
            placeholders[placeholder] = match.group()
            start, end = match.start() + offset, match.end() + offset
            modified_text = modified_text[:start] + placeholder + modified_text[end:]
            offset += len(placeholder) - len(match.group())

        # Split and restore URLs
        chunks = self._basic_split(modified_text)
        return (
            [
                chunk.replace(placeholder, url)
                for chunk in chunks
                for placeholder, url in placeholders.items()
            ]
            if placeholders
            else chunks
        )

    def _basic_split(self, text: str) -> List[str]:
        """Basic text splitting with overlap"""
        if len(text) <= self.chunk_size:
            return [text]

        chunks = []
        separators = ["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " "]

        start = 0
        while start < len(text):
            end = min(start + self.chunk_size, len(text))

            # Find best split point
            if end < len(text):
                for sep in separators:
                    split_pos = text.rfind(sep, start, end)
                    if split_pos > start:
                        end = split_pos + len(sep)
                        break

            chunks.append(text[start:end].strip())
            start = max(start + 1, end - self.chunk_overlap)

        return [chunk for chunk in chunks if chunk]


def parse_markdown(content: str, file_path: str) -> List[MarkdownSection]:
    """Parse markdown into hierarchical sections"""
    md = MarkdownIt("commonmark", {"breaks": True, "html": True})
    tokens = md.parse(content)
    lines = content.split("\n")

    sections = []
    headers = [""] * 6  # h1-h6 tracking
    current_content = []
    line_idx = 0

    for token in tokens:
        if token.type == "heading_open":
            # Save previous section
            if current_content:
                active_headers = [h for h in headers if h]
                sections.append(
                    MarkdownSection(
                        content="\n".join(current_content).strip(),
                        headers=active_headers.copy(),
                        level=len(active_headers),
                        file_path=file_path,
                    )
                )
                current_content = []

            # Update header hierarchy
            level = int(token.tag[1]) - 1
            next_idx = tokens.index(token) + 1
            header_text = (
                tokens[next_idx].content
                if next_idx < len(tokens) and tokens[next_idx].type == "inline"
                else ""
            )

            headers[level:] = [header_text] + [""] * (5 - level)

        # Collect content lines
        if token.map and line_idx < token.map[1]:
            current_content.extend(lines[line_idx : token.map[1]])
            line_idx = token.map[1]

    # Add remaining content
    current_content.extend(lines[line_idx:])
    if current_content:
        active_headers = [h for h in headers if h]
        sections.append(
            MarkdownSection(
                content="\n".join(current_content).strip(),
                headers=active_headers.copy(),
                level=len(active_headers),
                file_path=file_path,
            )
        )

    return [s for s in sections if s.content.strip()]


async def process_file(
    file_path: Path,
    embedder: GeminiEmbedder,
    splitter: URLAwareTextSplitter,
    progress: Progress = None,
    chunk_task=None,
) -> List[EmbeddingChunk]:
    """Process single markdown file"""
    try:
        content = file_path.read_text(encoding="utf-8")
        sections = parse_markdown(content, str(file_path))
    except Exception as e:
        console.print(f"[red]Error reading {file_path}: {e}[/red]")
        return []

    if not sections:
        return []

    chunks = []
    console.print(f"[cyan]üìÑ {file_path.name} ({len(sections)} sections)[/cyan]")

    for i, section in enumerate(sections):
        if not section.content.strip():
            if progress and chunk_task:
                progress.update(chunk_task, advance=1)
            continue

        topic_path = " > ".join(section.headers) if section.headers else "Root"
        full_text = f"""Topic: {topic_path}
File: {section.file_path}

{section.content}"""

        text_chunks = splitter.split_text(full_text)
        console.print(f"  üìù Section {i + 1} ({topic_path}): {len(text_chunks)} chunks")

        for j, chunk_text in enumerate(text_chunks):
            embedding = await embedder.embed_text(chunk_text)
            if embedding:
                chunks.append(
                    EmbeddingChunk(
                        text=chunk_text,
                        metadata={
                            "topic_path": topic_path,
                            "headers": section.headers,
                            "section_index": i,
                            "chunk_index": j,
                        },
                        embedding=embedding,
                    )
                )

        if progress and chunk_task:
            progress.update(chunk_task, advance=1)

    return chunks


def save_embeddings(chunks: List[EmbeddingChunk], output_path: Path) -> None:
    """Save embeddings to NPZ format"""
    if not chunks:
        error_exit("No chunks to save")

    valid_chunks = [c for c in chunks if c.embedding]
    if not valid_chunks:
        error_exit("No valid embeddings to save")

    np.savez_compressed(
        output_path,
        embeddings=np.array([c.embedding for c in valid_chunks], dtype=np.float32),
        texts=np.array([c.text for c in valid_chunks], dtype=object),
        topic_paths=np.array(
            [c.metadata.get("topic_path", "") for c in valid_chunks], dtype=object
        ),
        headers=np.array(
            [" | ".join(c.metadata.get("headers", [])) for c in valid_chunks],
            dtype=object,
        ),
    )


@click.group(name="course", help="Embed course markdown data")
def cli() -> None:
    pass


@cli.command()
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(exists=True, path_type=Path),
    default="data/course",
    help="Directory with markdown files",
)
@click.option(
    "--output-file",
    "-o",
    type=click.Path(path_type=Path),
    default="embeddings/course.npz",
    help="Output embeddings file",
)
@click.option("--chunk-size", default=1500, help="Max chunk size")
@click.option("--chunk-overlap", default=200, help="Chunk overlap")
def embed(
    input_dir: Path, output_file: Path, chunk_size: int, chunk_overlap: int
) -> None:
    """Embed course markdown files using Gemini"""

    try:
        settings = Settings()
    except Exception as e:
        error_exit(f"Config error: {e}")

    output_file.parent.mkdir(parents=True, exist_ok=True)

    async def run():
        console.print("[bold blue]üîç Analyzing input directory...[/bold blue]")

        md_files = list(input_dir.glob("**/*.md"))
        if not md_files:
            error_exit(f"No markdown files in {input_dir}")

        # Calculate total size
        total_size = sum(f.stat().st_size for f in md_files) / (1024 * 1024)

        # Show summary
        console.print("\n[bold yellow]üìã Embedding Summary[/bold yellow]")
        console.print(f"üìÅ Input directory: {input_dir}")
        console.print(f"üìÑ Files to process: {len(md_files)}")
        console.print(f"üìè Total size: {total_size:.2f} MB")
        console.print(f"‚öôÔ∏è  Chunk size: {chunk_size} chars")
        console.print(f"üîó Chunk overlap: {chunk_overlap} chars")
        console.print(f"üíæ Output: {output_file}")

        if not click.confirm("\nüöÄ Continue with embedding?", default=True):
            console.print("[yellow]‚ùå Cancelled by user[/yellow]")
            return

        console.print("\n[bold blue]üöÄ Starting embedding...[/bold blue]")

        embedder = GeminiEmbedder(settings.genai_api_key)
        splitter = URLAwareTextSplitter(chunk_size, chunk_overlap)

        all_chunks = []
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            # Calculate total chunks estimate (rough)
            total_sections = sum(
                len(parse_markdown(f.read_text(encoding="utf-8"), str(f)))
                for f in md_files
            )

            chunk_task = progress.add_task(
                "[yellow]Processing chunks...", total=total_sections
            )
            file_task = progress.add_task(
                "[cyan]Processing files...", total=len(md_files)
            )

            for file_path in md_files:
                progress.update(
                    file_task, description=f"[cyan]Processing {file_path.name}..."
                )
                chunks = await process_file(
                    file_path, embedder, splitter, progress, chunk_task
                )
                all_chunks.extend(chunks)
                progress.update(file_task, advance=1)

        successful = len([c for c in all_chunks if c.embedding])
        console.print(f"[green]‚úÖ Embedded {successful} chunks[/green]")

        save_embeddings(all_chunks, output_file)
        size_mb = output_file.stat().st_size / (1024 * 1024)
        console.print(f"[blue]üíæ Saved: {output_file} ({size_mb:.2f} MB)[/blue]")

    asyncio.run(run())


if __name__ == "__main__":
    cli()

# cli/embed/discourse copy.py
import asyncio
import json
import time
from pathlib import Path
from typing import Any
from typing import Dict
from typing import List
from typing import Optional

import numpy as np
import rich_click as click
from google import generativeai as genai
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pydantic import BaseModel
from pydantic import Field
from pydantic import ValidationError
from pydantic_settings import BaseSettings
from pydantic_settings import SettingsConfigDict
from rich.console import Console
from rich.progress import BarColumn
from rich.progress import Progress
from rich.progress import SpinnerColumn
from rich.progress import TaskProgressColumn
from rich.progress import TextColumn
from rich.progress import TimeElapsedColumn

from cli.utils import error_exit
from cli.utils import print_error
from cli.utils import print_warning

console = Console()


class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")
    genai_api_key: str


class PostData(BaseModel):
    username: str
    created_at: str
    content: str
    url: str
    images: List[Dict[str, str]] = Field(default_factory=list)


class TopicData(BaseModel):
    topic_id: int
    topic_title: str
    topic_url: str
    posts: List[PostData]


class EmbeddingChunk(BaseModel):
    text: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None


class SmartTextSplitter:
    def __init__(self, chunk_size: int = 1500, chunk_overlap: int = 200):
        self.splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " ", ""],
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False,
        )

    def split_text(self, text: str) -> List[str]:
        return self.splitter.split_text(text)


class GeminiEmbedder:
    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = "text-embedding-004"
        self.request_times: List[float] = []

    async def _rate_limit(self) -> None:
        """Rate limiting for Gemini free tier (1500 RPM = 25 RPS)"""
        now = time.time()
        self.request_times = [t for t in self.request_times if now - t < 60]

        if len(self.request_times) >= 24:  # Stay under 25 RPS
            wait_time = 60 - (now - self.request_times[0]) + 1
            if wait_time > 0:
                await asyncio.sleep(wait_time)

        await asyncio.sleep(0.05)  # Base delay
        self.request_times.append(now)

    async def embed_text(self, text: str) -> List[float]:
        """Embed text using Gemini's embedding model with rate limiting"""
        for attempt in range(3):
            try:
                await self._rate_limit()

                result = await asyncio.to_thread(
                    genai.embed_content,
                    model=self.model,
                    content=text,
                    task_type="retrieval_document",
                )
                return result["embedding"]

            except Exception as e:
                error_msg = str(e).lower()
                if "429" in error_msg or "quota" in error_msg:
                    wait_time = 30 + (attempt * 20)
                    print_warning(
                        f"Rate limited (attempt {attempt + 1}/3), waiting {wait_time}s"
                    )
                    await asyncio.sleep(wait_time)
                    continue
                elif attempt < 2:
                    await asyncio.sleep(5 + (attempt * 5))
                    continue
                else:
                    print_error(f"Embedding failed: {str(e)[:80]}")
                    return []
        return []


async def process_json_file(
    file_path: Path, embedder: GeminiEmbedder, splitter: SmartTextSplitter
) -> List[EmbeddingChunk]:
    """Process a single JSON file and return embedding chunks"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        topic_data = TopicData(**data)
        chunks = []

        console.print(
            f"[cyan]üìÑ Processing: {file_path.name} ({len(topic_data.posts)} posts)[/cyan]"
        )

        for post_idx, post in enumerate(topic_data.posts):
            if not post.content.strip():
                console.print(f"  ‚ö†Ô∏è  Post {post_idx + 1}: Skipping empty content")
                continue

            # Insert image context inline
            content = post.content
            for img in post.images:
                img_url = img.get("url", "")
                context = img.get("context", "")
                if img_url in content and context:
                    content = content.replace(
                        img_url, f"{img_url}\n[Image Context: {context}]"
                    )

            # Build embedding text inline
            full_text = f"""Topic: {topic_data.topic_title}
Topic URL: {topic_data.topic_url}
Post URL: {post.url}
Author: {post.username}
Created: {post.created_at}

Content:
{content}"""

            text_chunks = splitter.split_text(full_text)
            console.print(f"  üìù Post {post_idx + 1}: {len(text_chunks)} chunks")

            for chunk_text in text_chunks:
                embedding = await embedder.embed_text(chunk_text)

                if embedding:
                    chunks.append(
                        EmbeddingChunk(
                            text=chunk_text,
                            metadata={
                                "topic_title": topic_data.topic_title,
                                "post_url": post.url,
                            },
                            embedding=embedding,
                        )
                    )

        return chunks

    except Exception as e:
        print_error(f"Error processing {file_path}: {e}")
        return []


def save_embeddings_as_npz(chunks: List[EmbeddingChunk], output_path: Path) -> None:
    """Save embeddings to NPZ format (legacy fallback)"""
    if not chunks:
        error_exit("No chunks to save")

    # Separate embeddings and essential metadata only
    embeddings = []
    texts = []
    topic_titles = []
    post_urls = []

    for chunk in chunks:
        if chunk.embedding:
            embeddings.append(chunk.embedding)
            texts.append(chunk.text)
            topic_titles.append(chunk.metadata.get("topic_title", ""))
            post_urls.append(chunk.metadata.get("post_url", ""))

    if not embeddings:
        error_exit("No successful embeddings to save")

    # Convert to numpy arrays
    embeddings_array = np.array(embeddings, dtype=np.float32)

    # Save as NPZ with minimal metadata
    np.savez_compressed(
        output_path,
        embeddings=embeddings_array,
        texts=np.array(texts, dtype=object),
        topic_titles=np.array(topic_titles, dtype=object),
        post_urls=np.array(post_urls, dtype=object),
    )


@click.group(name="discourse", help="Embed Discourse data")
def cli() -> None:
    pass


@cli.command()
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(exists=True, path_type=Path),
    default="data/discourse",
    help="Directory containing JSON files",
)
@click.option(
    "--output-file",
    "-o",
    type=click.Path(path_type=Path),
    default="embeddings/discourse.npz",
    help="Output file for embeddings",
)
@click.option(
    "--chunk-size", default=1500, help="Maximum chunk size for text splitting"
)
@click.option("--chunk-overlap", default=200, help="Overlap between chunks")
def embed(
    input_dir: Path,
    output_file: Path,
    chunk_size: int,
    chunk_overlap: int,
) -> None:
    """Embed Discourse JSON files using Gemini's embedding model"""

    try:
        settings = Settings()
    except ValidationError as e:
        error_exit(f"Configuration error: {e}")

    output_file.parent.mkdir(parents=True, exist_ok=True)

    async def run_embedding():
        console.print(
            "[bold blue]üöÄ Starting Discourse embedding process...[/bold blue]"
        )

        embedder = GeminiEmbedder(settings.genai_api_key)
        splitter = SmartTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        json_files = list(input_dir.glob("*.json"))
        if not json_files:
            error_exit(f"No JSON files found in {input_dir}")

        console.print(f"[green]Found {len(json_files)} files to process[/green]")

        all_chunks = []

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task = progress.add_task("[cyan]Processing files...", total=len(json_files))

            for file_path in json_files:
                chunks = await process_json_file(file_path, embedder, splitter)
                all_chunks.extend(chunks)
                progress.update(task, advance=1)

        successful_chunks = len([c for c in all_chunks if c.embedding])
        console.print(
            f"[green]‚úÖ Successfully embedded {successful_chunks} chunks[/green]"
        )

        console.print("[cyan]üíæ Saving to NPZ format...[/cyan]")
        save_embeddings_as_npz(all_chunks, output_file)

        size_mb = output_file.stat().st_size / (1024 * 1024)
        console.print(f"[blue]üìÅ Saved: {output_file} ({size_mb:.2f} MB)[/blue]")

    asyncio.run(run_embedding())


if __name__ == "__main__":
    cli()

# cli/embed/discourse.py
import asyncio
import json
import re
import time
from pathlib import Path
from typing import Any
from typing import Dict
from typing import List
from typing import Optional

import numpy as np
import rich_click as click
from google import generativeai as genai
from pydantic import BaseModel
from pydantic import Field
from pydantic import ValidationError
from pydantic_settings import BaseSettings
from pydantic_settings import SettingsConfigDict
from rich.console import Console
from rich.progress import BarColumn
from rich.progress import Progress
from rich.progress import SpinnerColumn
from rich.progress import TaskProgressColumn
from rich.progress import TextColumn
from rich.progress import TimeElapsedColumn

from cli.utils import error_exit
from cli.utils import print_error
from cli.utils import print_warning

console = Console()


class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")
    genai_api_key: str


class PostData(BaseModel):
    username: str
    created_at: str
    content: str
    url: str
    images: List[Dict[str, str]] = Field(default_factory=list)


class TopicData(BaseModel):
    topic_id: int
    topic_title: str
    topic_url: str
    posts: List[PostData]


class EmbeddingChunk(BaseModel):
    text: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None


class URLAwareTextSplitter:
    """Text splitter preserving URLs as atomic units"""

    def __init__(self, chunk_size: int = 1500, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.url_pattern = re.compile(
            r"(?:https?://[^\s\[\]()]+|ftp://[^\s\[\]()]+|file://[^\s\[\]()]+|"
            r"\[[^\]]*\]\([^)]+\)|<[^>]+>)",
            re.IGNORECASE,
        )

    def split_text(self, text: str) -> List[str]:
        """Split text while preserving URLs"""
        urls = list(self.url_pattern.finditer(text))
        if not urls:
            return self._basic_split(text)

        # Replace URLs with placeholders
        placeholders = {}
        modified_text = text
        offset = 0

        for i, match in enumerate(urls):
            placeholder = f"__URL_{i}__"
            placeholders[placeholder] = match.group()
            start, end = match.start() + offset, match.end() + offset
            modified_text = modified_text[:start] + placeholder + modified_text[end:]
            offset += len(placeholder) - len(match.group())

        # Split and restore URLs
        chunks = self._basic_split(modified_text)
        return [self._restore_urls(chunk, placeholders) for chunk in chunks]

    def _restore_urls(self, chunk: str, placeholders: dict) -> str:
        """Restore URLs in chunk"""
        for placeholder, url in placeholders.items():
            chunk = chunk.replace(placeholder, url)
        return chunk

    def _basic_split(self, text: str) -> List[str]:
        """Basic text splitting with overlap"""
        if len(text) <= self.chunk_size:
            return [text]

        chunks = []
        separators = ["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " "]

        start = 0
        while start < len(text):
            end = min(start + self.chunk_size, len(text))

            # Find best split point
            if end < len(text):
                for sep in separators:
                    split_pos = text.rfind(sep, start, end)
                    if split_pos > start:
                        end = split_pos + len(sep)
                        break

            chunks.append(text[start:end].strip())
            start = max(start + 1, end - self.chunk_overlap)

        return [chunk for chunk in chunks if chunk]


class GeminiEmbedder:
    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = "text-embedding-004"
        self.request_times: List[float] = []

    async def _rate_limit(self) -> None:
        """Rate limiting for Gemini free tier (1500 RPM = 25 RPS)"""
        now = time.time()
        self.request_times = [t for t in self.request_times if now - t < 60]

        if len(self.request_times) >= 24:  # Stay under 25 RPS
            wait_time = 60 - (now - self.request_times[0]) + 1
            if wait_time > 0:
                await asyncio.sleep(wait_time)

        await asyncio.sleep(0.05)  # Base delay
        self.request_times.append(now)

    async def embed_text(self, text: str) -> List[float]:
        """Embed text using Gemini with rate limiting and retries"""
        for attempt in range(3):
            try:
                await self._rate_limit()
                result = await asyncio.to_thread(
                    genai.embed_content,
                    model=self.model,
                    content=text,
                    task_type="retrieval_document",
                )
                return result["embedding"]

            except Exception as e:
                error_msg = str(e).lower()
                if "429" in error_msg or "quota" in error_msg:
                    wait_time = 30 + (attempt * 20)
                    print_warning(
                        f"Rate limited (attempt {attempt + 1}/3), waiting {wait_time}s"
                    )
                    await asyncio.sleep(wait_time)
                elif attempt < 2:
                    await asyncio.sleep(5 + (attempt * 5))
                else:
                    print_error(f"Embedding failed: {str(e)[:80]}")
                    return []
        return []


async def process_json_file(
    file_path: Path, embedder: GeminiEmbedder, splitter: URLAwareTextSplitter
) -> List[EmbeddingChunk]:
    """Process single JSON file"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        topic_data = TopicData(**data)
    except Exception as e:
        print_error(f"Error processing {file_path}: {e}")
        return []

    chunks = []
    console.print(f"[cyan]üìÑ {file_path.name} ({len(topic_data.posts)} posts)[/cyan]")

    for post in topic_data.posts:
        if not post.content.strip():
            continue

        # Add image context inline
        content = post.content
        for img in post.images:
            if (img_url := img.get("url")) and (context := img.get("context")):
                content = content.replace(img_url, f"{img_url}\n[Image: {context}]")

        full_text = f"""Topic: {topic_data.topic_title}
Author: {post.username}
Created: {post.created_at}
URL: {post.url}

{content}"""

        text_chunks = splitter.split_text(full_text)

        for chunk_text in text_chunks:
            embedding = await embedder.embed_text(chunk_text)
            if embedding:
                chunks.append(
                    EmbeddingChunk(
                        text=chunk_text,
                        metadata={
                            "topic_title": topic_data.topic_title,
                            "post_url": post.url,
                        },
                        embedding=embedding,
                    )
                )

    return chunks


def save_embeddings(chunks: List[EmbeddingChunk], output_path: Path) -> None:
    """Save embeddings to NPZ format"""
    if not chunks:
        error_exit("No chunks to save")

    valid_chunks = [c for c in chunks if c.embedding]
    if not valid_chunks:
        error_exit("No valid embeddings to save")

    np.savez_compressed(
        output_path,
        embeddings=np.array([c.embedding for c in valid_chunks], dtype=np.float32),
        texts=np.array([c.text for c in valid_chunks], dtype=object),
        topic_titles=np.array(
            [c.metadata.get("topic_title", "") for c in valid_chunks], dtype=object
        ),
        post_urls=np.array(
            [c.metadata.get("post_url", "") for c in valid_chunks], dtype=object
        ),
    )


@click.group(name="discourse", help="Embed Discourse data")
def cli() -> None:
    pass


@cli.command()
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(exists=True, path_type=Path),
    default="data/discourse",
    help="Directory with JSON files",
)
@click.option(
    "--output-file",
    "-o",
    type=click.Path(path_type=Path),
    default="embeddings/discourse.npz",
    help="Output embeddings file",
)
@click.option("--chunk-size", default=1500, help="Max chunk size")
@click.option("--chunk-overlap", default=200, help="Chunk overlap")
def embed(
    input_dir: Path, output_file: Path, chunk_size: int, chunk_overlap: int
) -> None:
    """Embed Discourse JSON files using Gemini"""

    try:
        settings = Settings()
    except ValidationError as e:
        error_exit(f"Config error: {e}")

    output_file.parent.mkdir(parents=True, exist_ok=True)

    async def run():
        console.print("[bold blue]üîç Analyzing input directory...[/bold blue]")

        json_files = list(input_dir.glob("*.json"))
        if not json_files:
            error_exit(f"No JSON files in {input_dir}")

        # Calculate stats
        total_size = sum(f.stat().st_size for f in json_files) / (1024 * 1024)
        total_posts = 0

        for file_path in json_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    total_posts += len(data.get("posts", []))
            except Exception:
                continue

        # Show summary
        console.print("\n[bold yellow]üìã Embedding Summary[/bold yellow]")
        console.print(f"üìÅ Input directory: {input_dir}")
        console.print(f"üìÑ JSON files: {len(json_files)}")
        console.print(f"üí¨ Total posts: {total_posts}")
        console.print(f"üìè Total size: {total_size:.2f} MB")
        console.print(f"‚öôÔ∏è  Chunk size: {chunk_size} chars")
        console.print(f"üîó Chunk overlap: {chunk_overlap} chars")
        console.print(f"üíæ Output: {output_file}")

        if not click.confirm("\nüöÄ Continue with embedding?", default=True):
            console.print("[yellow]‚ùå Cancelled by user[/yellow]")
            return

        console.print("\n[bold blue]üöÄ Starting embedding...[/bold blue]")

        embedder = GeminiEmbedder(settings.genai_api_key)
        splitter = URLAwareTextSplitter(chunk_size, chunk_overlap)

        all_chunks = []
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task = progress.add_task("[cyan]Processing...", total=len(json_files))

            for file_path in json_files:
                chunks = await process_json_file(file_path, embedder, splitter)
                all_chunks.extend(chunks)
                progress.update(task, advance=1)

        successful = len([c for c in all_chunks if c.embedding])
        console.print(f"[green]‚úÖ Embedded {successful} chunks[/green]")

        save_embeddings(all_chunks, output_file)
        size_mb = output_file.stat().st_size / (1024 * 1024)
        console.print(f"[blue]üíæ Saved: {output_file} ({size_mb:.2f} MB)[/blue]")

    asyncio.run(run())


if __name__ == "__main__":
    cli()

# cli/utils/__init__.py
"""aws/utils/__init__.py - Shared utility functions for AWS CLI tool."""

import sys
from logging import INFO
from logging import Formatter
from logging import Logger
from logging import StreamHandler
from logging import getLogger
from typing import Any

from rich.console import Console
from rich.table import Table

console = Console()

STATUS_EMOJI = {
    "running": "‚úÖ",
    "stopped": "‚èπÔ∏è",
    "pending": "‚è≥",
    "stopping": "üõë",
    "terminated": "üíÄ",
    "shutting-down": "üîÑ",
    "healthy": "üíö",
    "unhealthy": "‚ù§Ô∏è",
    "warning": "‚ö†Ô∏è",
    "error": "üî•",
    "success": "üéâ",
    "failure": "‚ùå",
    "unknown": "‚ùì",
    "important": "üí°",
}


def print_table(
    title: str,
    columns: list[str],
    rows: list[list[Any]],
    column_styles: dict[str, str] | None = None,
    status_column: int | None = None,
) -> None:
    """Print a formatted table using Rich."""
    table = Table(title=title, show_header=True, header_style="bold")
    for col in columns:
        table.add_column(
            col,
            style=column_styles.get(col) if column_styles else None,
            no_wrap=True,
            overflow="ignore",
        )
    for row in rows:
        formatted_row = [str(cell or "") for cell in row]
        if status_column is not None and row[status_column].lower() in STATUS_EMOJI:
            formatted_row[status_column] = (
                f"{STATUS_EMOJI[row[status_column].lower()]} {formatted_row[status_column]}"
            )
        table.add_row(*formatted_row)
    console.print(table)


def print_success(message: str) -> None:
    console.print(f"{STATUS_EMOJI['success']}  {message}", style="bold green")


def print_warning(message: str) -> None:
    console.print(f"{STATUS_EMOJI['warning']}  {message}", style="bold yellow")


def print_error(message: str) -> None:
    console.print(f"{STATUS_EMOJI['error']}  {message}", style="bold red")


def print_important(message: str) -> None:
    console.print(f"{STATUS_EMOJI['important']}  {message}", style="bold blue")


def error_exit(message: str, exit_code: int = 1) -> None:
    print_error(message)
    sys.exit(exit_code)


def setup_logger(name: str) -> Logger:
    logger = getLogger(name)
    if not logger.handlers:
        handler = StreamHandler()
        handler.setFormatter(
            Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        )
        logger.addHandler(handler)
        logger.setLevel(INFO)
    return logger
################################################################################
./pyproject.toml
[project]
name = "tds-project-1"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
 "fastapi>=0.115.12",
 "google-generativeai>=0.8.5",
 "h5py>=3.14.0",
 "langchain>=0.3.25",
 "markdownify>=1.1.0",
 "numpy>=2.3.0",
 "pandas>=2.3.0",
 "pyarrow>=20.0.0",
 "pydantic>=2.11.5",
 "pydantic-ai>=0.2.12",
 "pydantic-settings>=2.9.1",
 "rich-click>=1.8.9",
 "toml>=0.10.2",
]

[dependency-groups]
dev = [ "black>=25.1.0", "isort>=6.0.1", "mypy>=1.16.0", "ruff>=0.11.12",]

[build-system]
requires = [ "setuptools>=61.0", "wheel",]
build-backend = "setuptools.build_meta"

[project.scripts]
cli = "cli:cli"
embed = "cli.embed:cli"
data = "cli.data:cli"

[tool.ruff]
target-version = "py312"
line-length = 88
extend-exclude = [ ".eggs", ".git", ".hg", ".mypy_cache", ".tox", ".venv", "_build", "buck-out", "build", "dist", "data",]

[tool.black]
line-length = 88
target-version = [ "py312",]
extend-exclude = "/(\n  # directories\n  \\.eggs\n  | \\.git\n  | \\.hg\n  | \\.mypy_cache\n  | \\.tox\n  | \\.venv\n  | _build\n  | buck-out\n  | build\n  | dist\n  | data\n)/\n"

[tool.isort]
profile = "black"
line_length = 88
force_single_line = true
split_on_trailing_comma = true
combine_as_imports = false
multi_line_output = 3
known_first_party = [ "cli",]
extend_skip_glob = [ ".eggs/*", ".git/*", ".hg/*", ".mypy_cache/*", ".tox/*", ".venv/*", "_build/*", "buck-out/*", "build/*", "dist/*", "data/*",]

[tool.mypy]
python_version = "3.12"
strict = true
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
exclude = [ "^.eggs/", "^.git/", "^.hg/", "^.mypy_cache/", "^.tox/", "^.venv/", "^_build/", "^buck-out/", "^build/", "^dist/", "^data/",]

[tool.ruff.lint]
select = [ "E", "F", "N", "W", "B", "C4", "PIE", "T20", "RET", "SIM", "ARG",]
ignore = [ "E501", "B904", "F404", "F401", "RET507", "PIE790",]

[tool.ruff.lint.isort]
known-first-party = [ "cli",]
force-single-line = true

[tool.setuptools.packages.find]
where = [ ".",]
include = [ "cli*",]
